================================================================================
File: "D:\other\Code\hackathon\server\.gitignore"
================================================================================
/output.txt/
/results/
/saved/
/scripts/helpers/__pycache__/
/scripts/model/__pycache__/
/data/~*

================================================================================
File: "D:\other\Code\hackathon\server\main.py"
================================================================================
from scripts.ProcessingAlgorithms.algorithms import LowerAlgorithm, AbbrExpandAlgorithm, NumsProcAlgorithm, SpellCheckAlgorithm
from scripts.ProcessingAlgorithms.texthandler import TextHandler
from scripts.helpers.helpers import generate_unique_words, generate_abbreviations, transform_labels
from scripts.helpers.xlsx_saver_reader import save_postproc_data_table, read_data_with_labels, transform_xlsx_into_csv, save_data_with_transformed_labels
from scripts.model.model import prepare_train_save_model
from scripts.model.model_usage import predict_labels
from scripts.model.model_precision import print_precision_scores
from pathlib import Path
import pandas as pd

# Настройка конвейера для предобратоки текста
def preproc_texts(data, preproc_dir):
    abbreviations = generate_abbreviations(preproc_dir, generate_sorted_abbreviations=False)
    text_handler = TextHandler()
    
    text_handler.add_algorithm(LowerAlgorithm())
    text_handler.add_algorithm(AbbrExpandAlgorithm(preproc_dir, abbreviations))
    text_handler.add_algorithm(NumsProcAlgorithm())
    # text_handler.add_algorithm(SpellCheckAlgorithm(preproc_dir)) # Не работает :<

    for i in range(len(data)):
        data[i] = text_handler.process_text(data[i])
    
    return data

def proc_labled_data(data_dir):
    data_with_labels = read_data_with_labels(data_dir)
    transformed_labels = transform_labels(data_with_labels)
    data_with_labels['label'] = transformed_labels

    print(data_with_labels.head())

    save_data_with_transformed_labels(data_with_labels, data_dir)

def main():
    # 1. Получение информации
    root = Path('.')
    data_dir = root / "data"
    preproc_dir = root / "text_info"
    
    # 2. Предобработка данных
    # data = pd.read_excel(data_dir / "data.xlsx").transpose().to_numpy()[0]

    # initial_data = data.copy()

    # data = preproc_texts(data, preproc_dir)
    # save_postproc_data_table(data, initial_data, data_dir)

    # 3. Перевод предобработанных данных в csv формат
    # transform_xlsx_into_csv(data_dir, 'postprocdata')

    # 4. Чтение и преобразование данных с готовыми метками
    # proc_labled_data(data_dir)

    # 5. получение готовой модели
    prepare_train_save_model(data_dir)

    # 6. Применение готовой модели
#     texts = ["""пахота зяби под многолетние травы
# по пу площадь за день 26 площадь с начала операции 514
# отделение 12 площадь за день 26 площадь с начала операции 247

# пахота зяби под сою
# по пу площадь за день 13 площадь с начала операции 1351
# отделение 16 площадь за день 13 площадь с начала операции 731

# предпосевная культура под озимая пшеница
# по пу площадь за день 56 площадь с начала операции 1071
# отделение 16 площадь за день 56 площадь с начала операции 585

# дискование сахарная свекла
# по пу площадь за день 85 площадь с начала операции 870
# отделение 17 площадь за день 85 площадь с начала операции 168

# второе дискование сахарная свекла под пшеница
# по пу площадь за день 123 площадь с начала операции 750
# отделение 12 площадь за день 123 площадь с начала операции 450

# второе дискование сои под озимая пшеница
# по пу площадь за день 82 площадь с начала операции 1989
# отделение 11 площадь за день 82 площадь с начала операции 993"""]

#     labels = predict_labels(texts)
#     print(labels)

    # 7. Проверка точности
    print_precision_scores(data_dir)



main()


================================================================================
File: "D:\other\Code\hackathon\server\README.md"
================================================================================
# Ход установки

1. Создание виртуальной среды

```bash
python -m venv .\.venv\
```

Потом здесь продолжим...

# Запуск тестового сервера

```bash
fastapi dev run_server.py
```

# Создание меток на данных

Можно это делать при помощи Label Studio

```bash
label-studio start
```

================================================================================
File: "D:\other\Code\hackathon\server\run_server.py"
================================================================================
from fastapi import FastAPI
from pydantic import BaseModel
import random
from datetime import datetime

app = FastAPI()

class AgroMessage(BaseModel):
    senderName: str
    telegramId: str
    text: str

class MessageClassification(BaseModel):
    data: datetime
    department: str
    operation: str
    plant: str
    perDay: int
    perOperation: int
    grosPerDay: int
    grosPerOperation: int


@app.post("/messages/proc_many")
async def process_messages(messages: list[AgroMessage]):
    responses = []
    for i in range(len(messages)):
        m = messages[i]
        response = MessageClassification(
            data=datetime.now(),
            department=f"Имя пользователя: {m.senderName}",
            operation=f"Telegram id: {m.telegramId}",
            plant=f"Полученное сообщение: {m.text}",
            perDay=random.randint(1, 100000),
            perOperation=random.randint(1, 100000),
            grosPerDay=random.randint(1, 100000),
            grosPerOperation=random.randint(1, 100000),
        )
        responses.append(response)
    
    return {"response": responses}


================================================================================
File: "D:\other\Code\hackathon\server\scripts\ProcessingAlgorithms\algorithms.py"
================================================================================
import pandas as pd
import numpy as np
import re
import tempfile
import os
from symspellpy import SymSpell, Verbosity
from pathlib import Path

# super class
class ProcessingAlgorithm:
    def process_text(self, text: str) -> str:
        return 'Processed text:' + text

# derived classes
# Удаляет символы \xa0 и переводит в нижний регистр
class LowerAlgorithm(ProcessingAlgorithm):
    def __init__(self):
        super().__init__()

    def process_text(self, text: str) -> str:
        return text.replace('\xa0', ' ').lower()

# Расширяет аббревиатуры
# Для работы нужна таблица аббревиатур /text_info/abbriviations.xlsx
class AbbrExpandAlgorithm(ProcessingAlgorithm):
    def __init__(self, preproc_dir: Path, abbreviations = None):
        super().__init__()
        if abbreviations is None:
            # reading abbreviations
            abbreviations = pd.read_excel(preproc_dir / 'abbriviations.xlsx').to_numpy()
            abbreviations = dict(abbreviations[:, 1:3].tolist())
            for key, val in list(abbreviations.items()):
                new_key = ''.join(key.split())
                abbreviations[new_key] = val
                new_key = ''.join(map(lambda s: s + '.', key.split()))
                abbreviations[new_key] = val
                new_key = ' '.join(map(lambda s: s + '.', key.split()))
                abbreviations[new_key] = val
            abbreviations = np.array(list(abbreviations.items()))
            abbreviations = abbreviations[abbreviations[:, 0].argsort()[::-1]]
            self.abbreviations = abbreviations

            # output = pd.DataFrame({'abbreviation': abbreviations[:,0], 'full_text': abbreviations[:,1]})
            # output.to_excel(preproc_dir / 'sortedabbriviations.xlsx')
        else:
            self.abbreviations = abbreviations

    def process_text(self, text: str) -> str:
        for pair in self.abbreviations:
            if text.startswith(pair[0] + ' '):
                text.replace(pair[0] + ' ', pair[1] + ' ', 1)
            text = text.replace(' ' + pair[0] + ' ', ' ' + pair[1] + ' ')
            text = text.replace(' ' + pair[0] + '\n', ' ' + pair[1] + '\n')
            text = text.replace('\n' + pair[0] + ' ', '\n' + pair[1] + ' ')
            text = text.replace('\n' + pair[0] + '\n', '\n' + pair[1] + '\n')
        return text

# Приводит цифры в более удобный формат
class NumsProcAlgorithm(ProcessingAlgorithm):
    def __init__(self):
        super().__init__()

    def process_text(self, text: str) -> str:
        pattern = r'(\d+/\d+)'
        matches = re.findall(pattern, text)
        for match in matches:
            per_day, per_operation = match.split('/')
            text = text.replace(match, f'площадь за день {per_day} площадь с начала операции {per_operation}')
        return text

# TODO: Исправить этот класс. Он всегда выдает неверные предположения.
class SpellCheckAlgorithm(ProcessingAlgorithm):
    def __init__(self, preproc_dir: Path):
        super().__init__()
        self.sym_spell = SymSpell()
        
        # Получение слов из edited_unique_words.xlsx
        words = pd.read_excel(preproc_dir / "edited_unique_words.xlsx").transpose().to_numpy()[1]
        
        dictionary_file = preproc_dir / "dictionary.txt"
        with open(dictionary_file, mode="w", encoding="utf-8") as dict_file:
            for word in words:
                dict_file.write(word + "\n")
        
        self.sym_spell.create_dictionary(str(dictionary_file))

    def process_text(self, text: str) -> str:
        correct_lines = []
        for line in text.split('\n'):
            correct_words = []
            for word in line.split(' '):
                if any(str.isdigit(ch) for ch in word):
                    correct_words.append(word)
                    continue
                susgestions = self.sym_spell.lookup(word, max_edit_distance=2, verbosity=Verbosity.CLOSEST)
                # print(f"All suggestions for '{word}': {[s.term for s in susgestions]}")
                if susgestions:
                    correct_words.append(susgestions[0].term)
                else:
                    correct_words.append(word)
            correct_lines.append(' '.join(correct_words))
        text = '\n'.join(correct_lines)
        return text



================================================================================
File: "D:\other\Code\hackathon\server\scripts\ProcessingAlgorithms\texthandler.py"
================================================================================
from .algorithms import ProcessingAlgorithm

class TextHandler:
    def __init__(self):
        self.algos = list()

    def add_algorithm(self, algo: ProcessingAlgorithm):
        self.algos.append(algo)
    
    def process_text(self, text: str) -> str:
        for algo in self.algos:
            text = algo.process_text(text)
        return text

================================================================================
File: "D:\other\Code\hackathon\server\scripts\model\model.py"
================================================================================
from transformers import BertTokenizerFast, BertForTokenClassification
from transformers import Trainer, TrainingArguments
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset

# Создание словаря меток
label_list = ['O', 'B-data', 'I-data', 'B-operation', 'I-operation', 'B-plant', 'I-plant', 'B-perDay', 'I-perDay', 'B-perOperation', 'I-perOperation', 'B-department', 'I-department']
label_to_id = {label: i for i, label in enumerate(label_list)}
id_to_label = {i: label for i, label in enumerate(label_list)}

def get_id_to_label():
    return id_to_label

# Загрузка предобученной модели и токенизатора
def download_pretrained_model():
    print("Загрузка предобученной модели по сети")
    tokenizer = BertTokenizerFast.from_pretrained("DeepPavlov/rubert-base-cased")
    model = BertForTokenClassification.from_pretrained(
        "DeepPavlov/rubert-base-cased",
        num_labels=len(label_list)
    )
    return tokenizer, model

# Обучение модели
def train_model(tokenized_data):
    print("Обучение модели")
    tokenizer, model = download_pretrained_model()

    # Определение аргументов обучения
    training_args = TrainingArguments(
        lr_scheduler_type="linear",
        output_dir="./results",
        eval_strategy="epoch",
        learning_rate=1e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=120,
        weight_decay=0.01,
    )

    # Создание объекта Trainer
    print(f"Тип tokenized_data['train']: {type(tokenized_data['train'])}")
    print(f"Тип tokenized_data['validation']: {type(tokenized_data['validation'])}")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_data["train"],
        eval_dataset=tokenized_data["validation"],
        tokenizer=tokenizer,
    )

    # Обучение модели
    trainer.train()
    return tokenizer, model

# Сохранение модели и токенизатора
def save_trained_model(model, tokenizer):
    print("Сохранение обученной модели и токенизатора")
    model.save_pretrained("./saved/model/")
    tokenizer.save_pretrained("./saved/tokenizer")

# Загрузка обученной модели и токенизатора
def load_pretrained_model():
    print("Загрузка обученной модели и токенизатора")
    tokenizer = BertTokenizerFast.from_pretrained("./saved/tokenizer")
    model = BertForTokenClassification.from_pretrained(
        "./saved/model/",
        num_labels=len(label_list)
    )
    return tokenizer, model

# Функция для выравнивания меток с токенами
def align_labels_with_tokens(labels, word_ids):
    # print("Начало работы моделя выравнивания меток с токенами")
    aligned_labels = []
    previous_word_idx = None
    for word_idx in word_ids:
        if word_idx is None: 
            aligned_labels.append(-100)
        elif word_idx != previous_word_idx: 
            # Проверка, что word_idx находится в допустимом диапазоне
            if 0 <= word_idx < len(labels):
                aligned_labels.append(label_to_id[labels[word_idx]]) # Преобразование меток в числа
            else:
                aligned_labels.append(-100)  # Игнорируем некорректные индексы
        else: 
            aligned_labels.append(-100)
        previous_word_idx = word_idx
    return aligned_labels

# Проверка меток
def check_labels(labels, num_labels):
    for label_seq in labels:
        for label in label_seq:
            if label >= num_labels or label < -100:
                raise ValueError(f"Некорректная метка: {label}. Допустимый диапазон: [-100, {num_labels - 1}]")

# Токенизация данных и выравнивание меток
def tokenize_and_align_labels(data, tokenizer):
    print("Начало работы моделя токенизации и выравнивания меток")

    # print(f"Работа метода tokenize_and_align_labels. Значение исходного data:\n{data.head()}")

    tokenized_inputs = tokenizer(
        data["postproc_data"].tolist(),
        truncation=True,
        padding='max_length',
        max_length=300,
        is_split_into_words=False, 
        return_tensors="pt",
    )

    # print(f"Работа метода tokenize_and_align_labels. Значение data после работы токенизатора:\n{data.head()}")
    
    all_labels = []
    for i, label in enumerate(data["label"]):
        # print(f"{i}: Значание data: {data.head()}")
        try:
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            labels = [item[1] for item in eval(label)]
            
            # print(f"Столбец postproc_data: {data['postproc_data'][:5]}")
            # print(f"Текст: {data['postproc_data'][i]}")
            # print(f"Метки: {labels}")
            # print(f"Word IDs: {word_ids}")
            
            aligned_labels = align_labels_with_tokens(labels, word_ids)
            all_labels.append(aligned_labels)
        except Exception as e:
            print(f"Ошибка при обработке строки {i}: {e}")
            all_labels.append([-100] * len(word_ids))  # Все метки игнорируются
    
    tokenized_inputs["labels"] = all_labels

    check_labels(all_labels, len(label_list))

    dataset = Dataset.from_dict(tokenized_inputs)
    return dataset

# Разделение данных на обучающую и валидационную выборки
def prepare_datasets(data_dir):
    print("Начало работы модуля разделения данных на выборки")

    # Загрузка данных
    data = pd.read_excel(data_dir / 'datawithbiolabels.xlsx')
    
    # Разделение на train и validation
    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)
    print(train_data.head())
    print(val_data.head())
    
    # Загрузка токенизатора
    tokenizer, _ = download_pretrained_model()
    
    # Токенизация данных
    train_tokenized = tokenize_and_align_labels(train_data, tokenizer)
    val_tokenized = tokenize_and_align_labels(val_data, tokenizer)
    
    tokenized_datasets = {
        "train": train_tokenized,
        "validation": val_tokenized
    }
    return tokenized_datasets

# Основная функция для запуска процесса
def prepare_train_save_model(data_dir):
    print("Начало работы основного метода создания модели.")
    
    # Подготовка данных
    tokenized_datasets = prepare_datasets(data_dir)
    
    # Обучение модели
    tokenizer, model = train_model(tokenized_datasets)
    
    # Сохранение модели
    save_trained_model(model, tokenizer)


================================================================================
File: "D:\other\Code\hackathon\server\scripts\model\model_precision.py"
================================================================================
import pandas as pd
from .model_usage import predict_labels
from seqeval.metrics import precision_score, recall_score, f1_score, classification_report

def print_precision_scores(data_dir):
    data_with_labels = pd.read_excel(data_dir / "datawithbiolabels.xlsx")

    predicted = predict_labels(data_with_labels['postproc_data'].to_list())
    true = data_with_labels['label'].to_list()
    for i in range(len(true)):
        true[i] = [item[1] for item in eval(true[i])]

    for i in range(len(predicted)):
        predicted[i] = predicted[i][:len(true[i])]
    
    for i in range(len(predicted)):
        true[i] = true[i][:len(predicted[i])]

    precision = precision_score(true, predicted)
    recall = recall_score(true, predicted)
    f1 = f1_score(true, predicted)

    print(f"Точность работы модели: {precision}")
    print(f"Полнота: {recall}")
    print(f"F1-score: {f1}")

    print(predicted)

================================================================================
File: "D:\other\Code\hackathon\server\scripts\model\model_usage.py"
================================================================================
from .model import get_id_to_label, load_pretrained_model
from transformers import BertTokenizerFast, BertForTokenClassification
import torch
from datasets import Dataset
import numpy as np

def predict_labels(texts):
    tokenizer, model = load_pretrained_model()
    id_to_label = get_id_to_label()

    # Токенизация входных текстов
    tokenized_inputs = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        is_split_into_words=False,
        max_length=300,
        return_tensors="pt"
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Создаем Dataset и устанавливаем формат torch
    dataset = Dataset.from_dict({
        'input_ids': tokenized_inputs['input_ids'],
        'attention_mask': tokenized_inputs['attention_mask']
    }).with_format("torch")

    all_labels = []
    for batch in dataset:
        # Подготавливаем входные данные
        inputs = {
            'input_ids': batch['input_ids'].unsqueeze(0).to(device),
            'attention_mask': batch['attention_mask'].unsqueeze(0).to(device)
        }

        # Получение предсказаний
        with torch.no_grad():
            outputs = model(**inputs)

        # Извлечение предсказанных логитов
        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()

        # Декодирование меток
        word_ids = tokenized_inputs.word_ids(batch_index=len(all_labels))
        labels = []
        previous_word_idx = None
        for word_idx, pred_id in zip(word_ids, predictions[0]):
            if word_idx is None:
                continue
            if word_idx != previous_word_idx:
                labels.append(id_to_label[pred_id])
            previous_word_idx = word_idx
        all_labels.append(labels)

    return all_labels


================================================================================
File: "D:\other\Code\hackathon\server\scripts\helpers\helpers.py"
================================================================================
from .xlsx_saver_reader import save_unique_words
import numpy as np
import pandas as pd

# В этом файле содержатся методы, помогающие в ходе работы

# Нужен для формирования словаря для SymSpell, проверяющего опечатки
# Уже должен быть сформирован файл postprocdata.xlsx
def generate_unique_words(data_dir, preproc_dir):
    data = pd.read_excel(data_dir / 'postprocdata.xlsx').transpose().to_numpy()[2]
    words = set()
    for line in data:
        for word in line.split():
            if any(str.isdigit(ch) for ch in word):
                continue
            words.add(word.lower())
    words = sorted(list(words))
    save_unique_words(words, preproc_dir)

# Служит для генерации словаря аббревиатур
# Метод может также генерировать список отсортированных аббревиатур, если есть необходимость
def generate_abbreviations(preproc_dir, generate_sorted_abbreviations = False):
    abbreviations = pd.read_excel(preproc_dir / 'abbriviations.xlsx').to_numpy()
    abbreviations = dict(abbreviations[:, 1:3].tolist())
    for key, val in list(abbreviations.items()):
        new_key = ''.join(key.split())
        abbreviations[new_key] = val
        new_key = ''.join(map(lambda s: s + '.', key.split()))
        abbreviations[new_key] = val
        new_key = ' '.join(map(lambda s: s + '.', key.split()))
        abbreviations[new_key] = val
    abbreviations = np.array(list(abbreviations.items()))
    abbreviations = abbreviations[abbreviations[:, 0].argsort()[::-1]]

    if generate_sorted_abbreviations:
        output = pd.DataFrame({'abbreviation': abbreviations[:,0], 'full_text': abbreviations[:,1]})
        output.to_excel(preproc_dir / 'sortedabbriviations.xlsx')
    return abbreviations

# Метод для преобразования label из label studio в label, читаемый ruBERT
def transform_labels(data):
    bio_data = []
    for _, row in data.iterrows():
        text = row["postproc_data"]
        labels = eval(row["label"])  # Преобразование JSON в список словарей
        tokens = text.split()
        bio_labels = ["O"] * len(tokens)
        
        for label in labels:
            start = label["start"]
            end = label["end"]
            entity = label["labels"][0]
            
            # Найдем токены, соответствующие метке
            token_indices = [i for i, tok in enumerate(tokens) if text.find(tok) >= start and text.find(tok) < end]
            if token_indices:
                bio_labels[token_indices[0]] = f"B-{entity}"
                for idx in token_indices[1:]:
                    bio_labels[idx] = f"I-{entity}"
        
        bio_data.append(list(zip(tokens, bio_labels)))
    
    return bio_data


================================================================================
File: "D:\other\Code\hackathon\server\scripts\helpers\xlsx_saver_reader.py"
================================================================================
import pandas as pd

# В этом файле содержатся методы для работы с xlsx

# Сохраняет таблицу с данными о сортированных аббривиатурах
def save_sorted_abbr_table(data, dir):
    output = pd.DataFrame({'abbreviation': data[:,0], 'full_text': data[:,1]})
    output.to_excel(dir / 'sortedabbriviations.xlsx')

# Сохраняет предобработанные данные
def save_postproc_data_table(data, initial_data, dir):
    output = pd.DataFrame({'initial': initial_data, 'postproc_data': data})
    output.to_excel(dir / "postprocdata.xlsx")

# сохраняет словарь уникальных слов
def save_unique_words(words, dir):
    output = pd.DataFrame({'word': words})
    output.to_excel(dir / 'unique_words.xlsx')

# метод используется для преобразования файла postprocdata.xlsx в csv формат, пригодный для label studio
def transform_xlsx_into_csv(data_dir, name):
    data = pd.read_excel(data_dir / (name + '.xlsx'))
    # print(data.head())
    data = data.drop(data.columns[[0, 1]], axis=1)
    data.to_csv(data_dir / (name + '.csv'), index=False)

# Данный метод нужен для чтения файла с готовыми метками
# Метки ставятся отдельно в label-studio
def read_data_with_labels(data_dir):
    data = pd.read_csv(data_dir / 'datawithlabels.csv')
    data = data.drop(data.columns[[0, 1, 2, 3, 5, 7]], axis=1)
    print(data.head())
    return data

def save_data_with_transformed_labels(data_df, data_dir):
    data_df.to_excel(data_dir / "datawithbiolabels.xlsx")



